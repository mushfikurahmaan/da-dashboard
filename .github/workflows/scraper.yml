name: Job Data Scraper

on:
  schedule:
    - cron: '0 */12 * * *'  # Run every 12 hours
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: write  # This is needed to allow the action to push changes

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r scraper/requirements.txt
      
      - name: Install Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
      
      - name: Install xvfb for headful mode
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb
      
      - name: Run scraper with xvfb
        run: |
          xvfb-run --auto-servernum python scraper/main.py
      
      - name: Commit and push if there are changes
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git add data/data.json
          # Check if there are changes to commit
          git diff --quiet && git diff --staged --quiet || (
            git commit -m "Update job data: $(date -u +'%Y-%m-%d %H:%M:%S')"
            git push
          ) 